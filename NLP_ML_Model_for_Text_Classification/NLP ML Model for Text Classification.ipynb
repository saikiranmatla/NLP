{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a ML Model for Text Classification using NLP features\n",
    "\n",
    "\n",
    "The task is to build a machine learning model to **classify** whether a particular tweet is **hate speech** or **not**.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. About the Dataset\n",
    "\n",
    "The dataset that i am using is of **Detecting Hate Speech** in people's tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('final_dataset_basicmlmodel.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to note**\n",
    " - **label** is the column that contains the target variable or the value that has to be predicted. 1 means it's a hate speech and 0 means it is not.\n",
    " - **tweet** is the column that contains the text of the tweet. This is the main data on which NLP techniques will be applied.\n",
    " \n",
    "Let's have a close look at some of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 .  â #ireland consumer price index (mom) climbed from previous 0.2% to 0.5% in may   #blog #silver #gold #forex\n",
      "2 . we are so selfish. #orlando #standwithorlando #pulseshooting #orlandoshooting #biggerproblems #selfish #heabreaking   #values #love #\n",
      "3 . i get to see my daddy today!!   #80days #gettingfed\n",
      "4 . ouch...junior is angryð#got7 #junior #yugyoem   #omg \n",
      "5 . i am thankful for having a paner. #thankful #positive     \n"
     ]
    }
   ],
   "source": [
    "for index, tweet in enumerate(dataset[\"tweet\"][10:15]):\n",
    "    print(index+1,\".\",tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :- Noise present in Tweets**\n",
    "\n",
    " - If you look closely, you'll see that there are many hashtags present in the tweets of the form `#` symbol followed by text. We particularly don't need the `#` symbol so we will clean it out.\n",
    " - Also, there are strange symbols like `â` and `ð` in tweet 4. This is actually `unicode` characters that is present in the dataset that we need to get rid of because they don't particularly add anything meaningful.\n",
    " - There are also numerals and percentages ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Data Cleaning\n",
    "import re\n",
    "\n",
    "#Clean text from noise\n",
    "def clean_text(text):\n",
    "    #Filter to allow only alphabets\n",
    "    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n",
    "    \n",
    "    #Remove Unicode characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    #Convert to lowercase to maintain consistency\n",
    "    text = text.lower()\n",
    "       \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['clean_text'] = dataset.tweet.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering\n",
    " - The machine learning model does not understand text directly, so we create numerical features that represent the  underlying text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exhaustive list of stopwords in the english language. We want to focus less on these so at some point will have to filter\n",
    "STOP_WORDS = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'also', 'am', 'an', 'and',\n",
    "              'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below',\n",
    "              'between', 'both', 'but', 'by', 'can', \"can't\", 'cannot', 'com', 'could', \"couldn't\", 'did',\n",
    "              \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'else', 'ever',\n",
    "              'few', 'for', 'from', 'further', 'get', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having',\n",
    "              'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "              \"how's\", 'however', 'http', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it',\n",
    "              \"it's\", 'its', 'itself', 'just', 'k', \"let's\", 'like', 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
    "              'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'otherwise', 'ought', 'our', 'ours',\n",
    "              'ourselves', 'out', 'over', 'own', 'r', 'same', 'shall', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\",\n",
    "              'should', \"shouldn't\", 'since', 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs',\n",
    "              'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\",\n",
    "              \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\",\n",
    "              'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
    "              \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\",\n",
    "              'www', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "#Generate word frequency\n",
    "def gen_freq(text):\n",
    "    #Will store the list of words\n",
    "    word_list = []\n",
    "\n",
    "    #Loop over all the tweets and extract words into word_list\n",
    "    for tw_words in text.split():\n",
    "        word_list.extend(tw_words)\n",
    "\n",
    "    #Create word frequencies using word_list\n",
    "    word_freq = pd.Series(word_list).value_counts()\n",
    "    \n",
    "    #Drop the stopwords during the frequency calculation\n",
    "    word_freq = word_freq.drop(STOP_WORDS, errors='ignore')\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "#Check whether a negation term is present in the text\n",
    "def any_neg(words):\n",
    "    for word in words:\n",
    "        if word in ['n', 'no', 'non', 'not'] or re.search(r\"\\wn't\", word):\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#Check whether one of the 100 rare words is present in the text\n",
    "def any_rare(words, rare_100):\n",
    "    for word in words:\n",
    "        if word in rare_100:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#Check whether prompt words are present\n",
    "def is_question(words):\n",
    "    for word in words:\n",
    "        if word in ['when', 'what', 'how', 'why', 'who']:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = gen_freq(dataset.clean_text.str)\n",
    "#100 most rare words in the dataset\n",
    "rare_100 = word_freq[-100:]\n",
    "#Number of words in a tweet\n",
    "dataset['word_count'] = dataset.clean_text.str.split().apply(lambda x: len(x))\n",
    "#Negation present or not\n",
    "dataset['any_neg'] = dataset.clean_text.str.split().apply(lambda x: any_neg(x))\n",
    "#Prompt present or not\n",
    "dataset['is_question'] = dataset.clean_text.str.split().apply(lambda x: is_question(x))\n",
    "#Any of the most 100 rare words present or not\n",
    "dataset['any_rare'] = dataset.clean_text.str.split().apply(lambda x: any_rare(x, rare_100))\n",
    "#Character count of the tweet\n",
    "dataset['char_count'] = dataset.clean_text.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user       3351\n",
       "amp         439\n",
       "love        320\n",
       "day         254\n",
       "trump       214\n",
       "happy       207\n",
       "will        191\n",
       "people      186\n",
       "new         171\n",
       "u           158\n",
       "libtard     149\n",
       "white       146\n",
       "black       142\n",
       "today       139\n",
       "one         130\n",
       "time        124\n",
       "good        124\n",
       "now         121\n",
       "life        112\n",
       "us          111\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 20 common words are\n",
    "gen_freq(dataset.clean_text.str)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>any_neg</th>\n",
       "      <th>is_question</th>\n",
       "      <th>any_rare</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>user when a father is dysfunctional and is s...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>user  user thanks for  lyft credit i can't us...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model   i love u take with u all the time in ...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide  society now     motivation</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3      0                                bihday your majesty   \n",
       "3   4      0  #model   i love u take with u all the time in ...   \n",
       "4   5      0             factsguide: society now    #motivation   \n",
       "\n",
       "                                          clean_text  word_count  any_neg  \\\n",
       "0    user when a father is dysfunctional and is s...          18        0   \n",
       "1   user  user thanks for  lyft credit i can't us...          19        1   \n",
       "2                                bihday your majesty           3        0   \n",
       "3   model   i love u take with u all the time in ...          12        0   \n",
       "4             factsguide  society now     motivation           4        0   \n",
       "\n",
       "   is_question  any_rare  char_count  \n",
       "0            1         0         102  \n",
       "1            0         0         122  \n",
       "2            0         0          21  \n",
       "3            0         0          86  \n",
       "4            0         0          39  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into Train-Test split \n",
    "This is a pretty regular practice in Machine Learning . It's just a way of testing your model's performance on unseen or Test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset[['word_count', 'any_neg', 'any_rare', 'char_count', 'is_question']]\n",
    "y = dataset.label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train an ML model for Text Classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Initialize GaussianNB classifier\n",
    "model = GaussianNB()\n",
    "#Fit the model on the train dataset\n",
    "model = model.fit(X_train, y_train)\n",
    "#Make predictions on the test dataset\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate the ML model\n",
    "\n",
    "It is time to train the model on previously unseen data: **X_test** and **y_test** sets that i created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion\n",
    "\n",
    "I used very basic NLP features, the classification accuracy and f1 scores aren't that impressive. My goal of doing this  exercise is to make myself familiar with the model building process and how to build a text classification model.\n",
    "\n",
    "###  Thank you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
